{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtain Pokemon images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Obtain the images from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d thedagger/pokemon-generation-one -p data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/images/pokemon-generation-one.zip -d data/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "from utils.evalutation import evaluate_model\n",
    "from utils.visualization import plot_accuracy_and_loss\n",
    "from utils.data_augmentation import get_data_augmentation\n",
    "from utils.callbacks import get_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = Path('data/images/dataset')\n",
    "new_base_dir = Path(original_dir.parent / 'splitted_dataset')\n",
    "\n",
    "train_dir = new_base_dir / 'train'\n",
    "val_dir = new_base_dir / 'val'\n",
    "test_dir = new_base_dir / 'test'\n",
    "\n",
    "data_augmentation = get_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Show some images with its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_folders = [f for f in os.listdir(original_dir)]\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    random_pokemon = random.choice(pokemon_folders)\n",
    "    pokemon_images = os.listdir(os.path.join(original_dir, random_pokemon))\n",
    "    random_image = random.choice(pokemon_images)\n",
    "    image_path = os.path.join(original_dir, random_pokemon, random_image)\n",
    "    \n",
    "    img = mpimg.imread(image_path)\n",
    "    ax[i].imshow(img)\n",
    "    ax[i].set_title(random_pokemon)\n",
    "    ax[i].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 See the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_counts = {}\n",
    "for folder in pokemon_folders:\n",
    "    folder_path = os.path.join(original_dir, folder) # Obtiene la ruta \n",
    "    image_files = [f for f in os.listdir(folder_path)]\n",
    "    image_counts[folder] = len(image_files)\n",
    "\n",
    "image_counts = dict(sorted(image_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_20_pokemon = list(image_counts.keys())[:20]\n",
    "top_20_counts = list(image_counts.values())[:20]\n",
    "\n",
    "bottom_20_pokemon = list(image_counts.keys())[-20:]\n",
    "bottom_20_counts = list(image_counts.values())[-20:]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4)) \n",
    "\n",
    "ax1.bar(top_20_pokemon, top_20_counts, color='green')\n",
    "ax1.set_title('Top 20 Pokémon with Most Images')\n",
    "ax1.set_xlabel('Pokémon')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.tick_params(axis='x', rotation=90) \n",
    "\n",
    "ax2.bar(bottom_20_pokemon, bottom_20_counts, color='red')\n",
    "ax2.set_title('Top 20 Pokémon with Fewest Images')\n",
    "ax2.set_xlabel('Pokémon')\n",
    "ax2.set_ylabel('Number of Images')\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Divide into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_iccp_if_png(image_path):\n",
    "    if image_path.suffix.lower() == '.png':\n",
    "        with Image.open(image_path) as img:\n",
    "            img.save(image_path, 'PNG', icc_profile=None)\n",
    "\n",
    "def copy_and_process_images(images, src_folder, dest_folder, pokemon):\n",
    "    os.makedirs(dest_folder / pokemon, exist_ok=True)\n",
    "    for img in images:\n",
    "        src_img_path = src_folder / img\n",
    "        dest_img_path = dest_folder / pokemon / img\n",
    "        shutil.copyfile(src_img_path, dest_img_path)\n",
    "        remove_iccp_if_png(dest_img_path)\n",
    "\n",
    "def split_dataset(train_ratio=0.7, val_ratio=0.2):\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(new_base_dir / split, exist_ok=True)\n",
    "\n",
    "    for pokemon in os.listdir(original_dir):\n",
    "        pokemon_path = original_dir / pokemon\n",
    "        images = os.listdir(pokemon_path)\n",
    "        random.shuffle(images)\n",
    "\n",
    "        total_images = len(images)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "\n",
    "        train_images = images[:train_count]\n",
    "        val_images = images[train_count:train_count + val_count]\n",
    "        test_images = images[train_count + val_count:]\n",
    "\n",
    "        copy_and_process_images(train_images, pokemon_path, new_base_dir / 'train', pokemon)\n",
    "        copy_and_process_images(val_images, pokemon_path, new_base_dir / 'val', pokemon)\n",
    "        copy_and_process_images(test_images, pokemon_path, new_base_dir / 'test', pokemon)\n",
    "\n",
    "split_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Load dataset from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "                batch_size=32,\n",
    "                image_size=(180, 180))\n",
    "\n",
    "val_dataset = image_dataset_from_directory(val_dir,\n",
    "                batch_size=32,\n",
    "                image_size=(180, 180))\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_dir,\n",
    "                batch_size=32,\n",
    "                image_size=(180, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(149, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"rmsprop\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=get_callbacks(\"models/basic_model.keras\", 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_and_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(\"models/basic_model.keras\", test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regularizate the basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(149, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=\"rmsprop\",\n",
    "            metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=get_callbacks(\"models/weight_regularized_model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(\"models/weight_regularized_model.keras\", test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3)\n",
    ")\n",
    "conv_base.trainable = False\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)                      \n",
    "x = keras.applications.vgg16.preprocess_input(x)   \n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(149, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=get_callbacks(\"models/feature_extraction_model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(\"models/feature_extraction_model.keras\", test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3)\n",
    ")\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)                      \n",
    "x = keras.applications.vgg16.preprocess_input(x)   \n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(149, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=get_callbacks(\"models/fine_tuned_model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(\"models/fine_tuned_model.keras\", test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokemon_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
